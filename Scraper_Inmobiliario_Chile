import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import urllib3
import re
import sys
import os
from datetime import datetime
from tqdm import tqdm

# --- SELENIUM ---
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager

# --- CONFIGURACI√ìN ---
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
HEADERS = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"}

# ==============================================================================
# üõ†Ô∏è FUNCIONES DE INTELIGENCIA (V51 - FECHAS HIST√ìRICAS)
# ==============================================================================

def extraer_fecha_entrega(texto):
    """
    Busca patrones de entrega, a√±os pasados/futuros o estado del proyecto.
    """
    t = str(texto).lower()
    
    # 1. Estados Clave (Tienen prioridad)
    if "inmediata" in t: return "Entrega Inmediata"
    if "en verde" in t: return "En Verde"
    if "en blanco" in t: return "En Blanco"
    if "pronta entrega" in t: return "Pronta Entrega"
    if "vendido" in t or "agotado" in t: return "Agotado/Vendido"
    if "futuro" in t or "lanzamiento" in t: return "Lanzamiento"
    
    # 2. Rangos de A√±os (Ej: "2018-2020", "2023 - 2025")
    match_rango = re.search(r"(20\d{2})\s*-\s*(20\d{2})", t)
    if match_rango:
        return f"{match_rango.group(1)}-{match_rango.group(2)}"

    # 3. Semestres/Trimestres (Cualquier a√±o 20xx)
    match_semestre = re.search(r"(1er|2do|primer|segundo|tercer|cuarto)\s*(sem|semestre|trimestre)?\s*(de)?\s*(20\d{2})", t)
    if match_semestre:
        return match_semestre.group(0).title()
        
    # 4. A√±o Suelto (Ej: "A√±o 2018", "Entrega 2020")
    # Busca 20xx precedido por palabras clave o espacios
    match_anio = re.search(r"(entrega|a√±o|desde|periodo|construcci√≥n)\s*(20\d{2})", t)
    if match_anio:
        return "A√±o " + match_anio.group(2)

    return "Consultar"

def deducir_region(texto_full):
    t = str(texto_full).lower()
    if any(x in t for x in ["metropolitana", "santiago", "lampa", "padre hurtado", "buin", "huechuraba", "san bernardo", "maip√∫", "florida", "√±u√±oa", "cisterna", "central", "independencia", "cerrillos", "macul", "san miguel", "providencia", "las condes", "vitacura", "lo barnechea", "puente alto", "san joaqu√≠n", "recoleta", "quilicura", "conchal√≠", "la reina", "pe√±alol√©n"]): return "Metropolitana"
    elif any(x in t for x in ["arica"]): return "Arica y Parinacota"
    elif any(x in t for x in ["antofagasta", "calama"]): return "Antofagasta"
    elif any(x in t for x in ["coquimbo", "ovalle", "serena", "vicu√±a"]): return "Coquimbo"
    elif any(x in t for x in ["valpara√≠so", "vi√±a", "concon", "conc√≥n", "quilpu√©", "villa alemana", "limache", "quillota", "marga marga", "papudo"]): return "Valpara√≠so"
    elif any(x in t for x in ["rancagua", "machal√≠", "rengo", "san fernando"]): return "O'Higgins"
    elif any(x in t for x in ["talca", "curic√≥", "linares", "maule"]): return "Maule"
    elif any(x in t for x in ["chill√°n", "√±uble", "san carlos"]): return "√ëuble"
    elif any(x in t for x in ["concepci√≥n", "coronel", "biob√≠o", "san pedro", "hualp√©n", "hualpen", "talcahuano", "los √°ngeles", "chiguayante"]): return "Biob√≠o"
    elif any(x in t for x in ["temuco", "araucan√≠a", "villarrica", "padre las casas"]): return "Araucan√≠a"
    elif any(x in t for x in ["valdivia", "los r√≠os", "la uni√≥n"]): return "Los R√≠os"
    elif any(x in t for x in ["puerto montt", "los lagos", "osorno", "puerto varas", "frutillar"]): return "Los Lagos"
    elif any(x in t for x in ["iquique", "hospicio"]): return "Tarapac√°"
    elif "coyhaique" in t or "ays√©n" in t: return "Ays√©n"
    elif "punta arenas" in t or "magallanes" in t: return "Magallanes"
    if "viii regi√≥n" in t: return "Biob√≠o"
    if "rm" in t: return "Metropolitana"
    if "iii regi√≥n" in t: return "Atacama"
    if "x regi√≥n" in t: return "Los Lagos"
    return ""

def limpiar_nombre_url(url):
    try:
        slug = url.split("/")[-1].split("?")[0]
        if not slug: slug = url.split("/")[-2].split("?")[0]
        slug = slug.replace("Form_", "").replace("form_", "").replace("proyecto-", "").replace("ficha-", "")
        slug = slug.replace("_", " ").replace("-", " ")
        slug = re.sub(r'(\D)(\d)', r'\1 \2', slug)
        return slug.title().strip()
    except: return "N/A"

def separar_palabras_pegadas(texto):
    if not texto: return ""
    return re.sub(r'([a-z])([A-Z])', r'\1 \2', texto)

def es_direccion_invalida(texto):
    t = str(texto).lower()
    if "apoquindo" in t and "4501" in t: return True
    if "ventas" in t or "horario" in t or "fono" in t: return True
    if len(t) < 4: return True
    return False

def iniciar_selenium(): 
    opts = Options()
    opts.add_argument("--headless")
    opts.add_argument("--disable-gpu")
    opts.add_argument("--log-level=3")
    return webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=opts)

def limpiar_basura_legal_oval(texto):
    if not texto: return "N/A"
    texto = texto.split('*')[0]
    texto = texto.split('Los bocetos')[0]
    texto = texto.split('Las im√°genes')[0]
    return texto.strip()

# ==============================================================================
# M√ìDULOS DE EXTRACCI√ìN (Requests)
# ==============================================================================

# 1. EBCO
def modulo_ebco(driver=None):
    links_totales = []
    pagina = 1
    while pagina <= 8:
        try:
            res = requests.get(f"https://ebco.cl/proyectos/construccion?page={pagina}", headers=HEADERS, verify=False, timeout=10)
            if res.status_code != 200: break
            soup = BeautifulSoup(res.text, 'html.parser')
            links_pag = []
            for a in soup.find_all('a', href=True):
                href = a['href']
                if "/proyectos/construccion/" in href and "page=" not in href:
                    if href.strip("/") == "/proyectos/construccion": continue
                    full = "https://ebco.cl" + href if href.startswith("/") else href
                    links_pag.append(full)
            if not links_pag: break
            links_totales.extend([l for l in list(set(links_pag)) if l not in links_totales])
            pagina += 1
        except: break
    
    todos = []
    for url in links_totales:
        data = {"Inmobiliaria": "CONSTRUCTORA EBCO S.A.", "Obra": "N/A", "Mandante": "", "Ubicaci√≥n": "N/A", "Regi√≥n": "", "Superficie": "", "Link": url}
        try:
            res = requests.get(url, headers=HEADERS, verify=False, timeout=10)
            soup = BeautifulSoup(res.text, 'html.parser')
            h1 = soup.find('h1'); data["Obra"] = h1.get_text(strip=True) if h1 else "N/A"
            txt = soup.get_text(" ", strip=True)
            m_mand = re.search(r"Mandante[:\s]+(.*?)(?=Ubicaci|Superficie|Im√°genes|$)", txt, re.IGNORECASE)
            if m_mand: data["Mandante"] = m_mand.group(1).strip()
            m_ubi = re.search(r"Ubicaci[o√≥]n[:\s]+(.*?)(?=Superficie|Mandante|Im√°genes|$)", txt, re.IGNORECASE)
            if m_ubi: 
                data["Ubicaci√≥n"] = m_ubi.group(1).strip()
                data["Regi√≥n"] = deducir_region(data["Ubicaci√≥n"])
            m_sup = re.search(r"Superficie[:\s]+(.*?)(?=Im√°genes|Ubicaci|Mandante|$)", txt, re.IGNORECASE)
            if m_sup: data["Superficie"] = m_sup.group(1).strip()
            
            # FECHA
            data["Estado/Entrega"] = extraer_fecha_entrega(txt)
            if "ejecuci√≥n" in txt.lower(): data["Estado/Entrega"] = "En Ejecuci√≥n"
            
            todos.append(data)
        except: pass
    return todos

# 2. INGEVEC
def modulo_ingevec(driver=None):
    try:
        res = requests.get("https://ingevecinmobiliaria.cl/proyectos-en-venta", headers=HEADERS, verify=False, timeout=15)
        soup = BeautifulSoup(res.text, 'html.parser')
        links = []
        for a in soup.find_all('a', href=True):
            h = a['href'].lower()
            if "ver proyecto" in a.get_text(" ", strip=True).lower() or "proyecto" in h or "ficha" in h or "form_" in h:
                if any(x in h for x in ["whatsapp", "facebook", "maps", "mailto"]): continue
                if "proyectos-en-venta" in h: continue
                full = "https://ingevecinmobiliaria.cl" + a['href'] if a['href'].startswith("/") else a['href']
                if full.startswith("http"): links.append(full)
        todos = []
        for url in list(set(links)):
            d = {"Inmobiliaria": "INMOBILIARIA Y CONSTRUCTORA INGEVEC S.A", "Obra": "N/A", "Mandante": "INGEVEC", "Ubicaci√≥n": "N/A", "Regi√≥n": "", "Superficie": "", "Link": url}
            try:
                r = requests.get(url, headers=HEADERS, verify=False, timeout=10)
                s = BeautifulSoup(r.text, 'html.parser')
                t = s.get_text(" ", strip=True)
                pat = {"Obra": r"Nombre[:\s]+(.*?)(?=Direcci|Dormitorios|$)", "Ubicaci√≥n": r"Direcci[o√≥]n[:\s]+(.*?)(?=Dormitorios|Ba√±os|Superficie|$)", "Superficie": r"Superficie total[:\s]+(.*?)(?=Estac|Estado|$)"}
                for k,v in pat.items():
                    m = re.search(v, t, re.IGNORECASE)
                    if m: d[k] = m.group(1).strip()[:100]
                if d["Ubicaci√≥n"] == "N/A":
                    for a in s.find_all('a', href=True):
                        if "maps" in a['href'] or "waze" in a['href']:
                            tx = a.get_text(strip=True)
                            if len(tx)>5 and any(c.isdigit() for c in tx) and "mapa" not in tx.lower():
                                d["Ubicaci√≥n"] = tx; break
                if d["Obra"] != "N/A": d["Obra"] = separar_palabras_pegadas(d["Obra"])
                if d["Obra"] == "N/A" or any(m in d["Obra"].lower() for m in ["conectividad", "entrega", "ganadora"]):
                    d["Obra"] = limpiar_nombre_url(url)
                if not d["Regi√≥n"]: d["Regi√≥n"] = deducir_region(d["Ubicaci√≥n"] + " " + d["Obra"])
                
                # FECHA
                d["Estado/Entrega"] = extraer_fecha_entrega(t)
                
                todos.append(d)
            except: pass
        return todos
    except: return []

# 3. SOCOVESA
def modulo_socovesa(driver=None):
    links_totales = []
    pagina = 1
    while pagina <= 10:
        try:
            res = requests.get(f"https://www.socovesa.cl/buscador-proyectos/?ciudad=&comuna=&tipologia=&paged={pagina}", headers=HEADERS, verify=False, timeout=15)
            if res.status_code != 200: break
            soup = BeautifulSoup(res.text, 'html.parser')
            links_pag = []
            for a in soup.find_all('a', href=True):
                if "/nuestros-proyectos/" in a['href'] and "page" not in a['href']:
                    href = a['href']
                    if href == "https://www.socovesa.cl/nuestros-proyectos/": continue
                    full = "https://www.socovesa.cl" + href if href.startswith("/") else href
                    links_pag.append(full)
            if not links_pag: break
            links_totales.extend([l for l in list(set(links_pag)) if l not in links_totales])
            pagina += 1
        except: break
    todos = []
    for url in links_totales:
        data = {"Inmobiliaria": "SOCOVESA INMOBILIARIA Y CONSTRUCCIONES", "Obra": "N/A", "Mandante": "SOCOVESA", "Ubicaci√≥n": "N/A", "Regi√≥n": "", "Superficie": "", "Link": url}
        try:
            res = requests.get(url, headers=HEADERS, verify=False, timeout=10)
            soup = BeautifulSoup(res.text, 'html.parser')
            txt = soup.get_text(" ", strip=True)
            h1 = soup.find('h1')
            if h1: data["Obra"] = h1.get_text(strip=True)
            match_dir = re.search(r"Direcci[o√≥]n[:\s]+(.*?)(?=Abrir|Waze|Google|$)", txt, re.IGNORECASE)
            if match_dir: data["Ubicaci√≥n"] = match_dir.group(1).strip()
            elif "Entrega" in txt:
                 match_geo = re.search(r"(Entrega|Venta).*?,(.*?),(.*?)$", txt, re.MULTILINE)
                 if match_geo: data["Ubicaci√≥n"] = f"{match_geo.group(2)}, {match_geo.group(3)}".strip()
            match_sup = re.search(r"Superficie[:\s]+(\d+[\.,]?\d*\s*m2?)", txt, re.IGNORECASE)
            if match_sup: data["Superficie"] = match_sup.group(1)
            if not data["Regi√≥n"]: data["Regi√≥n"] = deducir_region(data["Ubicaci√≥n"] + " " + txt)
            
            # FECHA
            data["Estado/Entrega"] = extraer_fecha_entrega(txt)
            
            todos.append(data)
        except: pass
    return todos

# 4. POCURO
def modulo_pocuro(driver):
    todos = []
    try:
        driver.get("https://www.pocuro.cl/proyectos")
        time.sleep(3)
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(2)
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        tarjetas = soup.find_all('a', class_="card-proyecto")
        if not tarjetas: tarjetas = soup.find_all('a', href=True)
        links = []
        for card in tarjetas:
            href = card.get('href')
            if href and "/proyecto/" in href:
                links.append("https://www.pocuro.cl" + href if href.startswith("/") else href)
        for link in list(set(links)):
            try:
                driver.get(link); time.sleep(1)
                s = BeautifulSoup(driver.page_source, 'html.parser')
                t = s.get_text(" ", strip=True)
                d = {"Inmobiliaria": "CONSTRUCTORA POCURO SPA", "Obra": "N/A", "Mandante": "POCURO", "Ubicaci√≥n": "N/A", "Regi√≥n": "", "Superficie": "", "Link": link}
                h1 = s.find('h1')
                d["Obra"] = h1.get_text(strip=True) if h1 else limpiar_nombre_url(link)
                m_dir = re.search(r"Direcci[o√≥]n[:\s]+(.*?)(?=Informaci|Hospital|Colegio|$)", t, re.IGNORECASE)
                if m_dir: d["Ubicaci√≥n"] = m_dir.group(1).strip().split("Abrir")[0]
                m_sup = re.search(r"(\d+[\.,]?\d*\s*m¬≤?)", t, re.IGNORECASE)
                if m_sup and float(m_sup.group(1).lower().replace("m¬≤","").replace("m2","").replace(",",".")) > 20: d["Superficie"] = m_sup.group(1)
                d["Regi√≥n"] = deducir_region(d["Ubicaci√≥n"] + " " + t)
                
                # FECHA
                d["Estado/Entrega"] = extraer_fecha_entrega(t)
                
                todos.append(d)
            except: continue
        return todos
    except: return []

# 5. OVAL
def modulo_oval(driver=None):
    try:
        res = requests.get("https://www.constructoraoval.cl/proyectos/", headers=HEADERS, verify=False, timeout=15)
        soup = BeautifulSoup(res.text, 'html.parser')
        links = list(set([btn.get('href') for btn in soup.find_all('a', string="Ver proyecto") if btn.get('href')]))
        todos = []
        for url in links:
            d = {"Inmobiliaria": "CONSTRUCTORA OVAL", "Obra": "N/A", "Mandante": "OVAL", "Ubicaci√≥n": "N/A", "Regi√≥n": "", "Superficie": "", "Link": url}
            try:
                r = requests.get(url, headers=HEADERS, verify=False, timeout=10)
                s = BeautifulSoup(r.text, 'html.parser')
                t = s.get_text(" ", strip=True)
                m_o = re.search(r"Obra[:\s]+(.*?)(?=Unidades|Destino|$)", t, re.IGNORECASE); 
                if m_o: d["Obra"] = m_o.group(1).strip()
                elif s.find('h1'): d["Obra"] = s.find('h1').get_text(strip=True)
                m_u = re.search(r"Ubicaci[o√≥]n[:\s]+(.*?)(?=Regi|Mandante|$)", t, re.IGNORECASE); 
                if m_u: d["Ubicaci√≥n"] = m_u.group(1).strip()
                
                # FIX OVAL EXCLUSIVO
                m_r = re.search(r"Regi[o√≥]n[:\s]+(.*?)(?=Previous|Next|$)", t, re.IGNORECASE); 
                if m_r: d["Regi√≥n"] = limpiar_basura_legal_oval(m_r.group(1))
                
                m_s = re.search(r"Superficie edificada[:\s]+(.*?)(?=Mandante|Destino|$)", t, re.IGNORECASE); 
                if m_s: d["Superficie"] = m_s.group(1).strip()
                
                # FECHA
                d["Estado/Entrega"] = extraer_fecha_entrega(t)
                
                todos.append(d)
            except: pass
        return todos
    except: return []

# 6. PACAL
def modulo_pacal(driver=None):
    try:
        res = requests.get("https://pacal.cl/proyectos-pacal/", headers=HEADERS, verify=False, timeout=15)
        soup = BeautifulSoup(res.text, 'html.parser')
        todos_datos = []
        parrafos = soup.find_all('p')
        for p in parrafos:
            txt = p.get_text(" ", strip=True)
            if "Direcci√≥n" in txt and "Ciudad" in txt:
                try:
                    data = {"Inmobiliaria": "CONSTRUCTORA PACAL S.A", "Obra": "N/A", "Mandante": "PACAL", "Ubicaci√≥n": "N/A", "Regi√≥n": "", "Superficie": "", "Link": "https://pacal.cl/proyectos-pacal/"}
                    match_dir = re.search(r"Direcci√≥n\s*:\s*(.*?)(?=Ciudad|$)", txt, re.IGNORECASE)
                    if match_dir: data["Ubicaci√≥n"] = match_dir.group(1).strip()
                    match_ciu = re.search(r"Ciudad\s*:\s*(.*?)(?=\+56|Tel|Ver|$)", txt, re.IGNORECASE)
                    if match_ciu:
                        c = match_ciu.group(1).strip()
                        data["Regi√≥n"] = deducir_region(c)
                        if not data["Regi√≥n"]: data["Regi√≥n"] = c
                    prev_a = p.find_previous('a', href=True)
                    i = 0
                    while prev_a and i < 5:
                        t_lnk = prev_a.get_text(strip=True)
                        if t_lnk and len(t_lnk)>3 and "constructora" not in t_lnk.lower():
                            data["Obra"] = t_lnk.replace("Condominio", "").replace("Edificio", "").strip()
                            data["Link"] = prev_a['href']
                            break
                        prev_a = prev_a.find_previous('a', href=True); i+=1
                    
                    # FECHA (Pacal la suele poner en otro <p> cercano, pero usaremos el radar)
                    data["Estado/Entrega"] = extraer_fecha_entrega(txt)
                    
                    todos_datos.append(data)
                except: continue
        seen = set(); uniq = []
        for d in todos_datos:
            if d['Obra'] not in seen and d['Obra']!="N/A": uniq.append(d); seen.add(d['Obra'])
        return uniq
    except: return []

# 7. SANTOLAYA
def modulo_santolaya(driver):
    url = "https://santolaya.cl/proyectos-venta"
    todos = []
    try:
        driver.get(url)
        time.sleep(3)
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(2)
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        btns = soup.find_all('a', class_=re.compile("btnVerProyecto"))
        if not btns: btns = soup.find_all('a', string=re.compile("VER PROYECTO", re.IGNORECASE))
        
        for btn in btns:
            try:
                data = {"Inmobiliaria": "CONSTRUCTORA SANTOLAYA", "Obra": "N/A", "Mandante": "SANTOLAYA", "Ubicaci√≥n": "N/A", "Regi√≥n": "", "Superficie": "", "Link": url}
                href = btn.get('href')
                if href: data["Link"] = "https://santolaya.cl/" + href.strip("/")
                nom = btn.find_previous('p', class_=re.compile("size18"))
                if nom: data["Obra"] = nom.get_text(" ", strip=True).replace("Edificio", "").strip()
                ubi = btn.find_previous('p', class_=re.compile("size13"))
                if ubi: data["Ubicaci√≥n"] = ubi.get_text(strip=True)
                data["Regi√≥n"] = deducir_region(data["Ubicaci√≥n"])
                sup = btn.find_previous(string=re.compile("M2", re.IGNORECASE))
                if sup: 
                    m = re.search(r"(\d+.*?[mM]2)", sup)
                    if m: data["Superficie"] = m.group(1)
                
                # FECHA
                banner = btn.find_previous(string=re.compile("Entrega|Nuevo", re.IGNORECASE))
                if banner: data["Estado/Entrega"] = banner.strip()
                else: data["Estado/Entrega"] = "Consultar"

                todos.append(data)
            except: continue
        seen = set(); uniq = []
        for d in todos:
            if d['Obra'] not in seen: uniq.append(d); seen.add(d['Obra'])
        return uniq
    except: return []

# 8. GUZMAN Y LARRAIN
def modulo_guzman(driver):
    url = "https://www.guzmanylarrain.com/proyectos/venta"
    proyectos = []
    try:
        driver.get(url)
        time.sleep(4)
        for i in range(2): driver.execute_script("window.scrollTo(0, document.body.scrollHeight);"); time.sleep(1)
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        tarjetas = soup.find_all('a', class_="bxPrList")
        for tarjeta in tarjetas:
            try:
                href = tarjeta.get('href'); 
                if not href: continue
                full_link = "https://www.guzmanylarrain.com" + href if href.startswith("/") else href
                data = {"Inmobiliaria": "CONSTRUCTORA GUZMAN Y LARRAI\u00ADN LTDA.", "Obra": "N/A", "Mandante": "GyL", "Ubicaci√≥n": "N/A", "Regi√≥n": "", "Superficie": "", "Link": full_link}
                h4 = tarjeta.find('h4'); data["Obra"] = h4.get_text(strip=True) if h4 else "N/A"
                ci = tarjeta.find('div', class_="titCiudad")
                if ci: 
                    c = ci.get_text(strip=True); data["Regi√≥n"] = deducir_region(c); data["Ubicaci√≥n"] = c
                txt_card = tarjeta.get_text(" ", strip=True)
                m = re.search(r"(\d+[\.,]?\d*)\s*m¬≤", txt_card)
                if m: data["Superficie"] = f"{m.group(0)}"
                
                # FECHA
                data["Estado/Entrega"] = extraer_fecha_entrega(txt_card)
                
                proyectos.append(data)
            except: continue
    except: return []
    finales = []
    for p in proyectos:
        try:
            driver.get(p['Link']); time.sleep(1.5)
            s = BeautifulSoup(driver.page_source, 'html.parser')
            icons = s.find_all('i', class_=re.compile("icn-moon|icon-location"))
            found = False
            for ic in icons:
                par = ic.find_parent('p')
                if par: 
                    txt = par.get_text(strip=True)
                    if len(txt)>5 and any(c.isdigit() for c in txt): 
                        p["Ubicaci√≥n"] = txt; found = True; break
            if not found:
                txt_det = s.get_text(" ", strip=True)
                m = re.search(r"Direcci[o√≥]n\s*[:\.]?\s*(.*?)(?=Horario|Tel√©fono|$)", txt_det, re.IGNORECASE)
                if m: p["Ubicaci√≥n"] = m.group(1).strip()
            finales.append(p)
        except: finales.append(p)
    return finales

# 9. PAZ
def modulo_paz(driver):
    url = "https://www.paz.cl/resultado-busqueda"
    base = []
    try:
        driver.get(url); time.sleep(5)
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);"); time.sleep(3)
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        tarjetas = soup.find_all('div', class_="paddColsProy")
        for c in tarjetas:
            try:
                lnk = c.find('a', class_="linkProyecto")
                if not lnk: continue
                full = "https://www.paz.cl" + lnk.get('href') if lnk.get('href').startswith("/") else lnk.get('href')
                d = {"Inmobiliaria": "CONSTRUCTORA PAZ SPA", "Obra": "N/A", "Mandante": "PAZ", "Ubicaci√≥n": "N/A", "Regi√≥n": "", "Superficie": "", "Link": full}
                nm = c.find('div', class_="NombreProyecto"); d["Obra"] = nm.get_text(strip=True) if nm else "N/A"
                co = c.find('div', class_="ComunaProyecto"); 
                if co: 
                    tc = co.get_text(strip=True); d["Regi√≥n"] = deducir_region(tc); d["_backup"] = tc
                    if not d["Regi√≥n"]: d["Regi√≥n"] = tc
                if d["Obra"]!="N/A" and any(x.isdigit() for x in d["Obra"]) and "etapa" not in d["Obra"].lower(): d["Ubicaci√≥n"] = d["Obra"].title()
                
                # FECHA
                txt_card = c.get_text(" ", strip=True)
                d["Estado/Entrega"] = extraer_fecha_entrega(txt_card)
                
                base.append(d)
            except: continue
    except: return []
    finales = []
    for p in base:
        if p["Ubicaci√≥n"] != "N/A": finales.append(p); continue
        try:
            driver.get(p['Link']); time.sleep(1.5)
            s = BeautifulSoup(driver.page_source, 'html.parser')
            cand = "N/A"
            sp = s.find_all('span', class_="direccionProyecto")
            for x in sp:
                t = x.get_text(strip=True)
                if len(t)>4 and not es_direccion_invalida(t): cand=t; break
            if es_direccion_invalida(cand):
                lb = s.find(string=re.compile("Direcci√≥n Edificio", re.IGNORECASE))
                if lb and lb.parent: cand = lb.parent.get_text(" ", strip=True).replace("Direcci√≥n Edificio:", "").strip()
            if es_direccion_invalida(cand):
                m = re.search(r"(Av\.|Avenida|Calle|Pasaje)\s+[A-Z√Å√â√ç√ì√ö√ëa-z\s\.]+\d+", s.get_text(" ", strip=True))
                if m: cand = m.group(0)
            
            if es_direccion_invalida(cand): cand = "N/A"
            p["Ubicaci√≥n"] = cand if cand != "N/A" else p.get("_backup", "N/A")
            if "_backup" in p: del p["_backup"]
            finales.append(p)
        except: finales.append(p)
    return finales

# 10. CARRAN
def modulo_carran(driver):
    try:
        driver.get("https://www.constructoracarran.cl/")
        time.sleep(4); driver.execute_script("window.scrollTo(0, document.body.scrollHeight);"); time.sleep(3)
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        tarjetas = soup.find_all('article', class_="project-card")
        todos = []
        for card in tarjetas:
            try:
                data = {"Inmobiliaria": "CONSTRUCTORA CARRAN S.A", "Obra": "N/A", "Mandante": "CARRAN", "Ubicaci√≥n": "N/A", "Regi√≥n": "", "Superficie": "", "Link": "https://www.constructoracarran.cl/#proyectos"}
                titulo = card.find('h3', class_="project-card__title")
                if titulo: data["Obra"] = titulo.get_text(strip=True)
                loc = card.find('p', class_="project-card__location")
                if loc:
                    tl = loc.get_text(strip=True); data["Ubicaci√≥n"] = tl; data["Regi√≥n"] = deducir_region(tl)
                l_m2 = card.find('span', class_="project-card__stat-label", string=re.compile("m¬≤", re.IGNORECASE))
                if l_m2:
                    v_m2 = l_m2.find_previous_sibling('span', class_="project-card__stat-value")
                    if v_m2: data["Superficie"] = v_m2.get_text(strip=True) + " m¬≤"
                
                # FECHA (Periodo)
                l_per = card.find('span', class_="project-card__stat-label", string=re.compile("Periodo", re.IGNORECASE))
                if l_per:
                    v_per = l_per.find_previous_sibling('span', class_="project-card__stat-value")
                    if v_per: data["Estado/Entrega"] = v_per.get_text(strip=True)
                else:
                    data["Estado/Entrega"] = "Consultar"

                if data["Obra"] != "N/A": todos.append(data)
            except: continue
        seen = set(); uniq = []
        for d in todos:
            if d['Obra'] not in seen: uniq.append(d); seen.add(d['Obra'])
        return uniq
    except: return []

# ==============================================================================
# üéØ EJECUCI√ìN
# ==============================================================================
def imprimir_menu():
    print("\n" + "="*50)
    print("   üèôÔ∏è  SCRAPER INMOBILIARIO CHILE - PANEL V51")
    print("="*50)
    print("1. Escanear TODO (Las 10 Inmobiliarias)")
    print("2. Solo R√°pidas (Requests)")
    print("3. Solo Lentas (Selenium)")
    print("4. Seleccionar una inmobiliaria espec√≠fica...")
    print("0. Salir")
    print("-" * 50)

def menu_especifico():
    print("\n1. EBCO\n2. INGEVEC\n3. SOCOVESA\n4. POCURO\n5. OVAL\n6. PACAL\n7. SANTOLAYA\n8. GUZM√ÅN\n9. PAZ\n10. CARR√ÅN")
    try: return int(input(">> Elige n√∫mero: "))
    except: return 0

def ejecutar_modulo(driver, num):
    if num == 1: return modulo_ebco(driver)
    elif num == 2: return modulo_ingevec(driver)
    elif num == 3: return modulo_socovesa(driver)
    elif num == 4: return modulo_pocuro(driver)
    elif num == 5: return modulo_oval(driver)
    elif num == 6: return modulo_pacal(driver)
    elif num == 7: return modulo_santolaya(driver)
    elif num == 8: return modulo_guzman(driver)
    elif num == 9: return modulo_paz(driver)
    elif num == 10: return modulo_carran(driver)
    return []

def guardar_datos(datos, modo_archivo):
    if not datos: print("‚ùå No hay datos."); return
    timestamp = datetime.now().strftime("%Y%m%d_%H%M")
    fecha_hoy = datetime.now().strftime("%d/%m/%Y")
    
    # Agregar fecha a todos los registros
    for d in datos: d["Fecha Scrapeo"] = fecha_hoy

    if modo_archivo == 1:
        df = pd.DataFrame(datos)
        df = df.drop_duplicates(subset=['Obra', 'Inmobiliaria'])
        # Reordenar columnas para que se vea bien
        cols = ["Fecha Scrapeo", "Inmobiliaria", "Obra", "Estado/Entrega", "Ubicaci√≥n", "Regi√≥n", "Superficie", "Link", "Mandante"]
        exist_cols = [c for c in cols if c in df.columns]
        df = df[exist_cols]
        nombre = f"Consolidado_Chile_{timestamp}.xlsx"
        df.to_excel(nombre, index=False)
        print(f"\nüíæ Archivo guardado: {nombre}")
    elif modo_archivo == 2:
        carpeta = f"Resultados_{timestamp}"
        os.makedirs(carpeta, exist_ok=True)
        df = pd.DataFrame(datos)
        grupos = df.groupby('Inmobiliaria')
        for nombre_inmo, grupo in grupos:
            safe_name = re.sub(r'[^a-zA-Z0-9]', '_', str(nombre_inmo))
            grupo.to_excel(f"{carpeta}/{safe_name}.xlsx", index=False)
            print(f"   -> {safe_name}.xlsx")

def main():
    while True:
        imprimir_menu()
        try: opcion = int(input(">> Opci√≥n: "))
        except: continue
        if opcion == 0: break

        print("\n--- SALIDA ---")
        print("1. Un solo Excel\n2. Multiples archivos")
        try: modo = int(input(">> Elige: "))
        except: modo = 1

        todos = []
        driver = None
        print("   üåê Iniciando motor...")
        driver = iniciar_selenium()
        
        try:
            if opcion == 1: # TODAS
                tareas = [("EBCO", modulo_ebco, False), ("INGEVEC", modulo_ingevec, False), ("SOCOVESA", modulo_socovesa, False), ("POCURO", modulo_pocuro, True), ("OVAL", modulo_oval, False), ("PACAL", modulo_pacal, False), ("SANTOLAYA", modulo_santolaya, True), ("GUZMAN", modulo_guzman, True), ("PAZ", modulo_paz, True), ("CARRAN", modulo_carran, True)]
                pbar = tqdm(tareas, desc="Progreso", unit="empresa")
                for nom, func, _ in pbar:
                    pbar.set_description(f"Procesando {nom}")
                    todos.extend(func(driver))
                    time.sleep(1)
            elif opcion == 2:
                modulos = [modulo_ebco, modulo_ingevec, modulo_socovesa, modulo_oval, modulo_pacal]
                for m in tqdm(modulos, desc="R√°pidas"): todos.extend(m(driver))
            elif opcion == 3:
                modulos = [modulo_pocuro, modulo_santolaya, modulo_guzman, modulo_paz, modulo_carran]
                for m in tqdm(modulos, desc="Lentas"): todos.extend(m(driver))
            elif opcion == 4:
                sel = menu_especifico()
                if sel > 0: todos.extend(ejecutar_modulo(driver, sel))

            guardar_datos(todos, modo)
            if opcion != 4: input("\n‚úÖ Listo. Enter para volver...")

        except Exception as e: print(f"\n‚ùå Error: {e}")
        finally:
            if driver: driver.quit()

if __name__ == "__main__":
    main()