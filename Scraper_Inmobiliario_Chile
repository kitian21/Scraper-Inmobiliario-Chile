import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import urllib3
import re
import sys
import os
from datetime import datetime
from tqdm import tqdm # Barra de progreso visual

# --- LIBRER√çAS DE SELENIUM (Navegaci√≥n Web Automatizada) ---
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager

# --- CONFIGURACI√ìN GLOBAL ---
# Desactivamos advertencias de certificados SSL (com√∫n en redes corporativas o scraping)
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# Simulamos ser un navegador real (Chrome en Windows) para evitar bloqueos b√°sicos
HEADERS = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"}

# ==============================================================================
# üõ†Ô∏è FUNCIONES DE APOYO Y LIMPIEZA
# ==============================================================================

def limpiar_basura_legal(texto):
    """
    Elimina textos legales largos que las inmobiliarias pegan junto a la regi√≥n.
    Ej: 'Valpara√≠so * Las im√°genes son referenciales...' -> 'Valpara√≠so'
    """
    if not texto: return "N/A"
    texto = texto.split('*')[0] # Corta en el primer asterisco
    texto = texto.split('Los bocetos')[0]
    texto = texto.split('Las im√°genes')[0]
    return texto.strip()

def deducir_region(texto_full):
    """
    Inteligencia b√°sica: Deduce la Regi√≥n administrativa (ej: 'Metropolitana')
    buscando palabras clave (ciudades/comunas) en un texto dado.
    """
    t = str(texto_full).lower()
    
    # Lista de palabras clave por regi√≥n
    if any(x in t for x in ["metropolitana", "santiago", "lampa", "padre hurtado", "buin", "huechuraba", "san bernardo", "maip√∫", "florida", "√±u√±oa", "cisterna", "central", "independencia", "cerrillos", "macul", "san miguel", "providencia", "las condes", "vitacura", "lo barnechea", "puente alto", "san joaqu√≠n", "recoleta", "quilicura", "conchal√≠", "la reina", "pe√±alol√©n"]): return "Metropolitana"
    elif any(x in t for x in ["arica"]): return "Arica y Parinacota"
    elif any(x in t for x in ["antofagasta", "calama"]): return "Antofagasta"
    elif any(x in t for x in ["coquimbo", "ovalle", "serena", "vicu√±a"]): return "Coquimbo"
    elif any(x in t for x in ["valpara√≠so", "vi√±a", "concon", "conc√≥n", "quilpu√©", "villa alemana", "limache", "quillota", "marga marga", "papudo"]): return "Valpara√≠so"
    elif any(x in t for x in ["rancagua", "machal√≠", "rengo", "san fernando"]): return "O'Higgins"
    elif any(x in t for x in ["talca", "curic√≥", "linares", "maule"]): return "Maule"
    elif any(x in t for x in ["chill√°n", "√±uble", "san carlos"]): return "√ëuble"
    elif any(x in t for x in ["concepci√≥n", "coronel", "biob√≠o", "san pedro", "hualp√©n", "hualpen", "talcahuano", "los √°ngeles", "chiguayante"]): return "Biob√≠o"
    elif any(x in t for x in ["temuco", "araucan√≠a", "villarrica", "padre las casas"]): return "Araucan√≠a"
    elif any(x in t for x in ["valdivia", "los r√≠os", "la uni√≥n"]): return "Los R√≠os"
    elif any(x in t for x in ["puerto montt", "los lagos", "osorno", "puerto varas", "frutillar"]): return "Los Lagos"
    elif any(x in t for x in ["iquique", "hospicio"]): return "Tarapac√°"
    elif "coyhaique" in t or "ays√©n" in t: return "Ays√©n"
    elif "punta arenas" in t or "magallanes" in t: return "Magallanes"
    
    # Fallbacks espec√≠ficos para abreviaciones raras
    if "viii regi√≥n" in t: return "Biob√≠o"
    if "rm" in t: return "Metropolitana"
    if "iii regi√≥n" in t: return "Atacama"
    if "x regi√≥n" in t: return "Los Lagos"
    return ""

def limpiar_nombre_url(url):
    """
    Si no encontramos el nombre en el HTML, lo reconstruimos desde el Link.
    Ej: .../proyecto-edificio-sur -> "Edificio Sur"
    """
    try:
        slug = url.split("/")[-1].split("?")[0] # Toma lo √∫ltimo del link
        if not slug: slug = url.split("/")[-2].split("?")[0] # Por si termina en /
        # Limpieza de prefijos comunes
        slug = slug.replace("Form_", "").replace("form_", "").replace("proyecto-", "").replace("ficha-", "")
        slug = slug.replace("_", " ").replace("-", " ")
        # Separa n√∫meros pegados a letras (Ej: "Edificio1" -> "Edificio 1")
        slug = re.sub(r'(\D)(\d)', r'\1 \2', slug)
        return slug.title().strip()
    except: return "N/A"

def separar_palabras_pegadas(texto):
    """Arregla errores de tipeo HTML: 'SantiagoCentro' -> 'Santiago Centro'"""
    if not texto: return ""
    return re.sub(r'([a-z])([A-Z])', r'\1 \2', texto)

def es_direccion_invalida(texto):
    """
    Filtro de seguridad para evitar guardar la direcci√≥n de la inmobiliaria
    en lugar de la direcci√≥n del proyecto.
    """
    t = str(texto).lower()
    if "apoquindo" in t and "4501" in t: return True # Oficina Paz
    if "ventas" in t or "horario" in t or "fono" in t: return True
    if len(t) < 4: return True # Direcciones muy cortas son basura
    return False

def iniciar_selenium(): 
    """Configura el navegador Chrome en modo invisible (Headless)"""
    opts = Options()
    opts.add_argument("--headless") # Comenta esto para ver el navegador abrirse
    opts.add_argument("--disable-gpu")
    opts.add_argument("--log-level=3") # Silenciar logs de consola
    return webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=opts)

# ==============================================================================
# M√ìDULOS DE EXTRACCI√ìN (Requests - R√°pidos)
# ==============================================================================

def modulo_ebco(driver=None):
    """
    EBCO usa paginaci√≥n simple (?page=1, ?page=2).
    Usamos Requests porque el HTML viene completo, no requiere JS.
    """
    links_totales = []
    pagina = 1
    # Bucle de paginaci√≥n (L√≠mite 8 para no saturar)
    while pagina <= 8:
        try:
            res = requests.get(f"https://ebco.cl/proyectos/construccion?page={pagina}", headers=HEADERS, verify=False, timeout=10)
            if res.status_code != 200: break
            soup = BeautifulSoup(res.text, 'html.parser')
            links_pag = []
            
            # Buscar links de proyectos
            for a in soup.find_all('a', href=True):
                href = a['href']
                if "/proyectos/construccion/" in href and "page=" not in href:
                    if href.strip("/") == "/proyectos/construccion": continue # Ignorar link a s√≠ mismo
                    full = "https://ebco.cl" + href if href.startswith("/") else href
                    links_pag.append(full)
            
            if not links_pag: break # Si no hay links, fin de paginaci√≥n
            
            # Agregar solo nuevos (evita duplicados)
            nuevos = [l for l in list(set(links_pag)) if l not in links_totales]
            if not nuevos: break
            links_totales.extend(nuevos)
            pagina += 1
        except: break
    
    # Extracci√≥n de datos
    todos = []
    for url in links_totales:
        data = {"Inmobiliaria": "EBCO", "Obra": "N/A", "Mandante": "", "Ubicaci√≥n": "N/A", "Regi√≥n": "", "Superficie": "", "Link": url}
        try:
            res = requests.get(url, headers=HEADERS, verify=False, timeout=10)
            soup = BeautifulSoup(res.text, 'html.parser')
            
            h1 = soup.find('h1'); data["Obra"] = h1.get_text(strip=True) if h1 else "N/A"
            txt = soup.get_text(" ", strip=True)
            
            # Regex para buscar pares Clave-Valor
            m_mand = re.search(r"Mandante[:\s]+(.*?)(?=Ubicaci|Superficie|Im√°genes|$)", txt, re.IGNORECASE)
            if m_mand: data["Mandante"] = m_mand.group(1).strip()
            
            m_ubi = re.search(r"Ubicaci[o√≥]n[:\s]+(.*?)(?=Superficie|Mandante|Im√°genes|$)", txt, re.IGNORECASE)
            if m_ubi: 
                data["Ubicaci√≥n"] = m_ubi.group(1).strip()
                data["Regi√≥n"] = deducir_region(data["Ubicaci√≥n"])
            
            m_sup = re.search(r"Superficie[:\s]+(.*?)(?=Im√°genes|Ubicaci|Mandante|$)", txt, re.IGNORECASE)
            if m_sup: data["Superficie"] = m_sup.group(1).strip()
            
            todos.append(data)
        except: pass
    return todos

def modulo_ingevec(driver=None):
    """
    Ingevec tiene una lista 'flat' (plana). Buscamos botones 'Ver Proyecto'.
    """
    try:
        res = requests.get("https://ingevecinmobiliaria.cl/proyectos-en-venta", headers=HEADERS, verify=False, timeout=15)
        soup = BeautifulSoup(res.text, 'html.parser')
        links = []
        for a in soup.find_all('a', href=True):
            h = a['href'].lower()
            # Filtro inteligente de links
            if "ver proyecto" in a.get_text(" ", strip=True).lower() or "proyecto" in h or "ficha" in h or "form_" in h:
                if any(x in h for x in ["whatsapp", "facebook", "maps", "mailto"]): continue
                if "proyectos-en-venta" in h: continue
                full = "https://ingevecinmobiliaria.cl" + a['href'] if a['href'].startswith("/") else a['href']
                if full.startswith("http"): links.append(full)
        
        todos = []
        for url in list(set(links)):
            d = {"Inmobiliaria": "INGEVEC", "Obra": "N/A", "Mandante": "INGEVEC", "Ubicaci√≥n": "N/A", "Regi√≥n": "", "Superficie": "", "Link": url}
            try:
                r = requests.get(url, headers=HEADERS, verify=False, timeout=10)
                s = BeautifulSoup(r.text, 'html.parser')
                t = s.get_text(" ", strip=True)
                
                # Regex para la ficha t√©cnica ordenada
                pat = {"Obra": r"Nombre[:\s]+(.*?)(?=Direcci|Dormitorios|$)", "Ubicaci√≥n": r"Direcci[o√≥]n[:\s]+(.*?)(?=Dormitorios|Ba√±os|Superficie|$)", "Superficie": r"Superficie total[:\s]+(.*?)(?=Estac|Estado|$)"}
                for k,v in pat.items():
                    m = re.search(v, t, re.IGNORECASE)
                    if m: d[k] = m.group(1).strip()[:100]
                
                # Rescate: Buscar link a Google Maps para sacar la direcci√≥n
                if d["Ubicaci√≥n"] == "N/A":
                    for a in s.find_all('a', href=True):
                        if "maps" in a['href'] or "waze" in a['href']:
                            tx = a.get_text(strip=True)
                            if len(tx)>5 and any(c.isdigit() for c in tx) and "mapa" not in tx.lower():
                                d["Ubicaci√≥n"] = tx; break
                
                if d["Obra"] != "N/A": d["Obra"] = separar_palabras_pegadas(d["Obra"])
                if d["Obra"] == "N/A" or any(m in d["Obra"].lower() for m in ["conectividad", "entrega", "ganadora"]):
                    d["Obra"] = limpiar_nombre_url(url)
                if not d["Regi√≥n"]: d["Regi√≥n"] = deducir_region(d["Ubicaci√≥n"] + " " + d["Obra"])
                todos.append(d)
            except: pass
        return todos
    except: return []

def modulo_socovesa(driver=None):
    """
    Socovesa usa par√°metros GET (?paged=X). F√°cil de iterar.
    """
    links_totales = []
    pagina = 1
    while pagina <= 10:
        try:
            res = requests.get(f"https://www.socovesa.cl/buscador-proyectos/?ciudad=&comuna=&tipologia=&paged={pagina}", headers=HEADERS, verify=False, timeout=15)
            if res.status_code != 200: break
            soup = BeautifulSoup(res.text, 'html.parser')
            links_pag = []
            for a in soup.find_all('a', href=True):
                if "/nuestros-proyectos/" in a['href'] and "page" not in a['href']:
                    href = a['href']
                    if href == "https://www.socovesa.cl/nuestros-proyectos/": continue
                    full = "https://www.socovesa.cl" + href if href.startswith("/") else href
                    links_pag.append(full)
            if not links_pag: break
            nuevos = [l for l in list(set(links_pag)) if l not in links_totales]
            if not nuevos: break
            links_totales.extend(nuevos)
            pagina += 1
        except: break
    
    todos = []
    for url in links_totales:
        data = {"Inmobiliaria": "SOCOVESA", "Obra": "N/A", "Mandante": "SOCOVESA", "Ubicaci√≥n": "N/A", "Regi√≥n": "", "Superficie": "", "Link": url}
        try:
            res = requests.get(url, headers=HEADERS, verify=False, timeout=10)
            soup = BeautifulSoup(res.text, 'html.parser')
            txt = soup.get_text(" ", strip=True)
            h1 = soup.find('h1')
            if h1: data["Obra"] = h1.get_text(strip=True)
            
            match_dir = re.search(r"Direcci[o√≥]n[:\s]+(.*?)(?=Abrir|Waze|Google|$)", txt, re.IGNORECASE)
            if match_dir: data["Ubicaci√≥n"] = match_dir.group(1).strip()
            elif "Entrega" in txt:
                 match_geo = re.search(r"(Entrega|Venta).*?,(.*?),(.*?)$", txt, re.MULTILINE)
                 if match_geo: data["Ubicaci√≥n"] = f"{match_geo.group(2)}, {match_geo.group(3)}".strip()
            match_sup = re.search(r"Superficie[:\s]+(\d+[\.,]?\d*\s*m2?)", txt, re.IGNORECASE)
            if match_sup: data["Superficie"] = match_sup.group(1)
            if not data["Regi√≥n"]: data["Regi√≥n"] = deducir_region(data["Ubicaci√≥n"] + " " + txt)
            todos.append(data)
        except: pass
    return todos

def modulo_oval(driver=None):
    """
    Oval tiene todo en una p√°gina. Busca enlaces que digan 'Ver proyecto'.
    """
    try:
        res = requests.get("https://www.constructoraoval.cl/proyectos/", headers=HEADERS, verify=False, timeout=15)
        soup = BeautifulSoup(res.text, 'html.parser')
        links = list(set([btn.get('href') for btn in soup.find_all('a', string="Ver proyecto") if btn.get('href')]))
        todos = []
        for url in links:
            d = {"Inmobiliaria": "OVAL", "Obra": "N/A", "Mandante": "OVAL", "Ubicaci√≥n": "N/A", "Regi√≥n": "", "Superficie": "", "Link": url}
            try:
                r = requests.get(url, headers=HEADERS, verify=False, timeout=10)
                s = BeautifulSoup(r.text, 'html.parser')
                t = s.get_text(" ", strip=True)
                
                # Regex para cada campo
                m_o = re.search(r"Obra[:\s]+(.*?)(?=Unidades|Destino|$)", t, re.IGNORECASE); 
                if m_o: d["Obra"] = m_o.group(1).strip()
                elif s.find('h1'): d["Obra"] = s.find('h1').get_text(strip=True)
                
                m_u = re.search(r"Ubicaci[o√≥]n[:\s]+(.*?)(?=Regi|Mandante|$)", t, re.IGNORECASE); 
                if m_u: d["Ubicaci√≥n"] = m_u.group(1).strip()
                
                # FIX V48: Limpieza de Regi√≥n (Basura legal)
                m_r = re.search(r"Regi[o√≥]n[:\s]+(.*?)(?=Previous|Next|$)", t, re.IGNORECASE); 
                if m_r: d["Regi√≥n"] = limpiar_basura_legal(m_r.group(1))
                
                m_s = re.search(r"Superficie edificada[:\s]+(.*?)(?=Mandante|Destino|$)", t, re.IGNORECASE); 
                if m_s: d["Superficie"] = m_s.group(1).strip()
                
                todos.append(d)
            except: pass
        return todos
    except: return []

def modulo_pacal(driver=None):
    """
    Pacal tiene TODA la info en la portada. Usamos extracci√≥n directa sin entrar a links.
    """
    try:
        res = requests.get("https://pacal.cl/proyectos-pacal/", headers=HEADERS, verify=False, timeout=15)
        soup = BeautifulSoup(res.text, 'html.parser')
        todos_datos = []
        # Buscamos p√°rrafos que tengan Direcci√≥n y Ciudad juntos
        parrafos = soup.find_all('p')
        for p in parrafos:
            txt = p.get_text(" ", strip=True)
            if "Direcci√≥n" in txt and "Ciudad" in txt:
                try:
                    data = {"Inmobiliaria": "PACAL", "Obra": "N/A", "Mandante": "PACAL", "Ubicaci√≥n": "N/A", "Regi√≥n": "", "Superficie": "", "Link": "https://pacal.cl/proyectos-pacal/"}
                    match_dir = re.search(r"Direcci√≥n\s*:\s*(.*?)(?=Ciudad|$)", txt, re.IGNORECASE)
                    if match_dir: data["Ubicaci√≥n"] = match_dir.group(1).strip()
                    match_ciu = re.search(r"Ciudad\s*:\s*(.*?)(?=\+56|Tel|Ver|$)", txt, re.IGNORECASE)
                    if match_ciu:
                        c = match_ciu.group(1).strip()
                        data["Regi√≥n"] = deducir_region(c)
                        if not data["Regi√≥n"]: data["Regi√≥n"] = c
                    
                    # Buscar nombre hacia atr√°s (el t√≠tulo est√° antes del p√°rrafo)
                    prev_a = p.find_previous('a', href=True)
                    i = 0
                    while prev_a and i < 5:
                        t_lnk = prev_a.get_text(strip=True)
                        if t_lnk and len(t_lnk)>3 and "constructora" not in t_lnk.lower():
                            data["Obra"] = t_lnk.replace("Condominio", "").replace("Edificio", "").strip()
                            data["Link"] = prev_a['href']
                            break
                        prev_a = prev_a.find_previous('a', href=True); i+=1
                    todos_datos.append(data)
                except: continue
        seen = set(); uniq = []
        for d in todos_datos:
            if d['Obra'] not in seen and d['Obra']!="N/A": uniq.append(d); seen.add(d['Obra'])
        return uniq
    except: return []

# ==============================================================================
# M√ìDULOS DE EXTRACCI√ìN (Selenium - Pesados)
# ==============================================================================

def modulo_pocuro(driver):
    """Pocuro carga con JS. Usamos Selenium para ver las tarjetas."""
    todos = []
    try:
        driver.get("https://www.pocuro.cl/proyectos")
        time.sleep(3)
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(2)
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        
        tarjetas = soup.find_all('a', class_="card-proyecto")
        if not tarjetas: tarjetas = soup.find_all('a', href=True)
        
        links = []
        for card in tarjetas:
            href = card.get('href')
            if href and "/proyecto/" in href:
                links.append("https://www.pocuro.cl" + href if href.startswith("/") else href)
        
        for link in list(set(links)):
            try:
                driver.get(link); time.sleep(1)
                s = BeautifulSoup(driver.page_source, 'html.parser')
                t = s.get_text(" ", strip=True)
                d = {"Inmobiliaria": "POCURO", "Obra": "N/A", "Mandante": "POCURO", "Ubicaci√≥n": "N/A", "Regi√≥n": "", "Superficie": "", "Link": link}
                h1 = s.find('h1')
                d["Obra"] = h1.get_text(strip=True) if h1 else limpiar_nombre_url(link)
                m_dir = re.search(r"Direcci[o√≥]n[:\s]+(.*?)(?=Informaci|Hospital|Colegio|$)", t, re.IGNORECASE)
                if m_dir: d["Ubicaci√≥n"] = m_dir.group(1).strip().split("Abrir")[0]
                m_sup = re.search(r"(\d+[\.,]?\d*\s*m¬≤?)", t, re.IGNORECASE)
                if m_sup and float(m_sup.group(1).lower().replace("m¬≤","").replace("m2","").replace(",",".")) > 20: d["Superficie"] = m_sup.group(1)
                d["Regi√≥n"] = deducir_region(d["Ubicaci√≥n"] + " " + t)
                todos.append(d)
            except: continue
        return todos
    except: return []

def modulo_santolaya(driver):
    """Santolaya usa JS. Buscamos botones 'VER PROYECTO'."""
    url = "https://santolaya.cl/proyectos-venta"
    todos = []
    try:
        driver.get(url)
        time.sleep(3)
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(2)
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        btns = soup.find_all('a', class_=re.compile("btnVerProyecto"))
        if not btns: btns = soup.find_all('a', string=re.compile("VER PROYECTO", re.IGNORECASE))
        
        for btn in btns:
            try:
                data = {"Inmobiliaria": "SANTOLAYA", "Obra": "N/A", "Mandante": "SANTOLAYA", "Ubicaci√≥n": "N/A", "Regi√≥n": "", "Superficie": "", "Link": url}
                href = btn.get('href')
                if href: data["Link"] = "https://santolaya.cl/" + href.strip("/")
                nom = btn.find_previous('p', class_=re.compile("size18"))
                if nom: data["Obra"] = nom.get_text(" ", strip=True).replace("Edificio", "").strip()
                ubi = btn.find_previous('p', class_=re.compile("size13"))
                if ubi: data["Ubicaci√≥n"] = ubi.get_text(strip=True)
                data["Regi√≥n"] = deducir_region(data["Ubicaci√≥n"])
                sup = btn.find_previous(string=re.compile("M2", re.IGNORECASE))
                if sup: 
                    m = re.search(r"(\d+.*?[mM]2)", sup)
                    if m: data["Superficie"] = m.group(1)
                todos.append(data)
            except: continue
        seen = set(); uniq = []
        for d in todos:
            if d['Obra'] not in seen: uniq.append(d); seen.add(d['Obra'])
        return uniq
    except: return []

def modulo_guzman(driver):
    """Guzman usa Angular. Extraemos de portada y entramos para ver direcci√≥n (icono)."""
    url = "https://www.guzmanylarrain.com/proyectos/venta"
    proyectos = []
    try:
        driver.get(url)
        time.sleep(4)
        for i in range(2): driver.execute_script("window.scrollTo(0, document.body.scrollHeight);"); time.sleep(1)
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        tarjetas = soup.find_all('a', class_="bxPrList")
        for tarjeta in tarjetas:
            try:
                href = tarjeta.get('href'); 
                if not href: continue
                full_link = "https://www.guzmanylarrain.com" + href if href.startswith("/") else href
                data = {"Inmobiliaria": "GUZMAN Y LARRAIN", "Obra": "N/A", "Mandante": "GyL", "Ubicaci√≥n": "N/A", "Regi√≥n": "", "Superficie": "", "Link": full_link}
                h4 = tarjeta.find('h4'); data["Obra"] = h4.get_text(strip=True) if h4 else "N/A"
                ci = tarjeta.find('div', class_="titCiudad")
                if ci: 
                    c = ci.get_text(strip=True); data["Regi√≥n"] = deducir_region(c); data["Ubicaci√≥n"] = c # Backup
                txt_card = tarjeta.get_text(" ", strip=True)
                m = re.search(r"(\d+[\.,]?\d*)\s*m¬≤", txt_card)
                if m: data["Superficie"] = f"{m.group(0)}"
                proyectos.append(data)
            except: continue
    except: return []
    finales = []
    for p in proyectos:
        try:
            driver.get(p['Link']); time.sleep(1.5)
            s = BeautifulSoup(driver.page_source, 'html.parser')
            # V35 Icon fix: buscar lunita o pin
            icons = s.find_all('i', class_=re.compile("icn-moon|icon-location"))
            found = False
            for ic in icons:
                par = ic.find_parent('p')
                if par: 
                    txt = par.get_text(strip=True)
                    if len(txt)>5 and any(c.isdigit() for c in txt): 
                        p["Ubicaci√≥n"] = txt; found = True; break
            if not found:
                txt_det = s.get_text(" ", strip=True)
                m = re.search(r"Direcci[o√≥]n\s*[:\.]?\s*(.*?)(?=Horario|Tel√©fono|$)", txt_det, re.IGNORECASE)
                if m: p["Ubicaci√≥n"] = m.group(1).strip()
            finales.append(p)
        except: finales.append(p)
    return finales

def modulo_paz(driver):
    """Paz usa Anti-Bot y direcciones dif√≠ciles. Usamos selectores CSS exactos y filtro de oficinas."""
    url = "https://www.paz.cl/resultado-busqueda"
    base = []
    try:
        driver.get(url); time.sleep(5)
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);"); time.sleep(3)
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        tarjetas = soup.find_all('div', class_="paddColsProy")
        for c in tarjetas:
            try:
                lnk = c.find('a', class_="linkProyecto")
                if not lnk: continue
                full = "https://www.paz.cl" + lnk.get('href') if lnk.get('href').startswith("/") else lnk.get('href')
                d = {"Inmobiliaria": "PAZ", "Obra": "N/A", "Mandante": "PAZ", "Ubicaci√≥n": "N/A", "Regi√≥n": "", "Superficie": "", "Link": full}
                nm = c.find('div', class_="NombreProyecto"); d["Obra"] = nm.get_text(strip=True) if nm else "N/A"
                co = c.find('div', class_="ComunaProyecto"); 
                if co: 
                    tc = co.get_text(strip=True); d["Regi√≥n"] = deducir_region(tc); d["_backup"] = tc
                    if not d["Regi√≥n"]: d["Regi√≥n"] = tc
                # Si el nombre tiene numeros, es la direcci√≥n (V40)
                if d["Obra"]!="N/A" and any(x.isdigit() for x in d["Obra"]) and "etapa" not in d["Obra"].lower(): d["Ubicaci√≥n"] = d["Obra"].title()
                base.append(d)
            except: continue
    except: return []
    finales = []
    for p in base:
        if p["Ubicaci√≥n"] != "N/A": finales.append(p); continue
        try:
            driver.get(p['Link']); time.sleep(1.5)
            s = BeautifulSoup(driver.page_source, 'html.parser')
            cand = "N/A"
            # V43 logic: buscar clase direccionProyecto
            sp = s.find_all('span', class_="direccionProyecto")
            for x in sp:
                t = x.get_text(strip=True)
                if len(t)>4 and not es_direccion_invalida(t): cand=t; break
            
            # Respaldo: Buscar label "Direcci√≥n Edificio"
            if es_direccion_invalida(cand):
                lb = s.find(string=re.compile("Direcci√≥n Edificio", re.IGNORECASE))
                if lb and lb.parent: cand = lb.parent.get_text(" ", strip=True).replace("Direcci√≥n Edificio:", "").strip()
            
            # Respaldo final: Regex
            if es_direccion_invalida(cand):
                m = re.search(r"(Av\.|Avenida|Calle|Pasaje)\s+[A-Z√Å√â√ç√ì√ö√ëa-z\s\.]+\d+", s.get_text(" ", strip=True))
                if m: cand = m.group(0)
            
            p["Ubicaci√≥n"] = cand if not es_direccion_invalida(cand) else p.get("_backup", "N/A")
            if "_backup" in p: del p["_backup"]
            finales.append(p)
        except: finales.append(p)
    return finales

def modulo_carran(driver):
    """Carran es One Page. Usamos Selenium para scroll y capturar tarjetas."""
    try:
        driver.get("https://www.constructoracarran.cl/")
        time.sleep(4); driver.execute_script("window.scrollTo(0, document.body.scrollHeight);"); time.sleep(3)
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        tarjetas = soup.find_all('article', class_="project-card")
        todos = []
        for card in tarjetas:
            try:
                data = {"Inmobiliaria": "CARRAN", "Obra": "N/A", "Mandante": "CARRAN", "Ubicaci√≥n": "N/A", "Regi√≥n": "", "Superficie": "", "Link": "https://www.constructoracarran.cl/#proyectos"}
                titulo = card.find('h3', class_="project-card__title")
                if titulo: data["Obra"] = titulo.get_text(strip=True)
                loc = card.find('p', class_="project-card__location")
                if loc:
                    tl = loc.get_text(strip=True); data["Ubicaci√≥n"] = tl; data["Regi√≥n"] = deducir_region(tl)
                l_m2 = card.find('span', class_="project-card__stat-label", string=re.compile("m¬≤", re.IGNORECASE))
                if l_m2:
                    v_m2 = l_m2.find_previous_sibling('span', class_="project-card__stat-value")
                    if v_m2: data["Superficie"] = v_m2.get_text(strip=True) + " m¬≤"
                if data["Obra"] != "N/A": todos.append(data)
            except: continue
        seen = set(); uniq = []
        for d in todos:
            if d['Obra'] not in seen: uniq.append(d); seen.add(d['Obra'])
        return uniq
    except: return []

# ==============================================================================
# üéØ MEN√ö Y EJECUCI√ìN
# ==============================================================================
def imprimir_menu():
    print("\n" + "="*50)
    print("   üèôÔ∏è  SCRAPER INMOBILIARIO CHILE - PANEL V48")
    print("="*50)
    print("1. Escanear TODO (Las 10 Inmobiliarias)")
    print("2. Solo R√°pidas (Requests: Oval, Ebco, Ingevec, Socovesa, Pacal)")
    print("3. Solo Lentas (Selenium: Pocuro, Santolaya, Guzman, Paz, Carran)")
    print("4. Seleccionar una inmobiliaria espec√≠fica...")
    print("0. Salir")
    print("-" * 50)

def menu_especifico():
    print("\n--- SELECCIONA UNA INMOBILIARIA ---")
    print("1. EBCO")
    print("2. INGEVEC")
    print("3. SOCOVESA")
    print("4. POCURO")
    print("5. OVAL")
    print("6. PACAL")
    print("7. SANTOLAYA")
    print("8. GUZM√ÅN Y LARRA√çN")
    print("9. PAZ")
    print("10. CARR√ÅN")
    try:
        return int(input(">> Elige n√∫mero: "))
    except: return 0

def ejecutar_modulo(driver, num):
    """Ejecuta un m√≥dulo espec√≠fico por n√∫mero."""
    if num == 1: return modulo_ebco(driver)
    elif num == 2: return modulo_ingevec(driver)
    elif num == 3: return modulo_socovesa(driver)
    elif num == 4: return modulo_pocuro(driver)
    elif num == 5: return modulo_oval(driver)
    elif num == 6: return modulo_pacal(driver)
    elif num == 7: return modulo_santolaya(driver)
    elif num == 8: return modulo_guzman(driver)
    elif num == 9: return modulo_paz(driver)
    elif num == 10: return modulo_carran(driver)
    return []

def guardar_datos(datos, modo_archivo):
    """Guarda los datos en Excel seg√∫n la preferencia del usuario."""
    if not datos:
        print("‚ùå No hay datos para guardar.")
        return

    timestamp = datetime.now().strftime("%Y%m%d_%H%M")
    
    if modo_archivo == 1:
        # Modo Consolidado
        df = pd.DataFrame(datos)
        df = df.drop_duplicates(subset=['Obra', 'Inmobiliaria'])
        nombre = f"Consolidado_Chile_{timestamp}.xlsx"
        df.to_excel(nombre, index=False)
        print(f"\nüíæ Archivo guardado: {nombre}")
        
    elif modo_archivo == 2:
        # Modo Separado
        carpeta = f"Resultados_{timestamp}"
        os.makedirs(carpeta, exist_ok=True)
        print(f"\nüìÇ Creando carpeta: {carpeta}")
        
        df = pd.DataFrame(datos)
        grupos = df.groupby('Inmobiliaria')
        
        for nombre_inmo, grupo in grupos:
            # Limpiamos nombre para archivo (sin caracteres raros)
            safe_name = re.sub(r'[^a-zA-Z0-9]', '_', str(nombre_inmo))
            archivo = f"{carpeta}/{safe_name}.xlsx"
            grupo.to_excel(archivo, index=False)
            print(f"   -> Guardado: {archivo}")

def main():
    while True:
        imprimir_menu()
        try:
            opcion = int(input(">> Ingresa tu opci√≥n: "))
        except:
            print("‚ùå Opci√≥n inv√°lida."); continue

        if opcion == 0: break

        # Preguntar formato de archivo
        print("\n--- FORMATO DE SALIDA ---")
        print("1. Un solo Excel consolidado")
        print("2. Multiples archivos (uno por inmobiliaria)")
        try:
            modo_archivo = int(input(">> Elige (1 o 2): "))
        except: modo_archivo = 1

        todos_los_datos = []
        driver = None
        
        # Siempre iniciamos driver al principio para simplificar la l√≥gica
        # (aunque algunas no lo usen, es m√°s seguro tenerlo disponible)
        print("   üåê Abriendo navegador (un momento)...")
        driver = iniciar_selenium()
        
        try:
            if opcion == 1: # TODAS
                print("\nüöÄ Iniciando escaneo completo...")
                # Lista de tareas EN EL ORDEN PEDIDO
                tareas = [
                    ("EBCO", modulo_ebco, False),
                    ("INGEVEC", modulo_ingevec, False),
                    ("SOCOVESA", modulo_socovesa, False),
                    ("POCURO", modulo_pocuro, True),
                    ("OVAL", modulo_oval, False),
                    ("PACAL", modulo_pacal, False),
                    ("SANTOLAYA", modulo_santolaya, True),
                    ("GUZMAN", modulo_guzman, True),
                    ("PAZ", modulo_paz, True),
                    ("CARRAN", modulo_carran, True)
                ]
                
                # Barra de progreso visual
                pbar = tqdm(tareas, desc="Progreso General", unit="empresa")
                
                for nombre, func, usa_driver in pbar:
                    pbar.set_description(f"Procesando {nombre}")
                    # Pasamos el driver a todas (las que no lo usan, lo ignoran)
                    res = func(driver)
                    todos_los_datos.extend(res)
                    time.sleep(1)

            elif opcion == 2: # R√ÅPIDAS
                modulos = [modulo_ebco, modulo_ingevec, modulo_socovesa, modulo_oval, modulo_pacal]
                for m in tqdm(modulos, desc="R√°pidas"):
                    todos_los_datos.extend(m(driver))

            elif opcion == 3: # LENTAS
                modulos = [modulo_pocuro, modulo_santolaya, modulo_guzman, modulo_paz, modulo_carran]
                for m in tqdm(modulos, desc="Lentas"):
                    todos_los_datos.extend(m(driver))

            elif opcion == 4: # ESPEC√çFICA
                sel = menu_especifico()
                if sel > 0:
                    todos_los_datos.extend(ejecutar_modulo(driver, sel))

            # Guardar
            guardar_datos(todos_los_datos, modo_archivo)
            
            if opcion != 4: 
                input("\n‚úÖ Proceso terminado. Presiona Enter para volver al men√∫...")

        except Exception as e:
            print(f"\n‚ùå Error: {e}")
        finally:
            if driver: 
                driver.quit()
                print("   üëã Navegador cerrado.")

if __name__ == "__main__":
    main()